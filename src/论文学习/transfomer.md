---
tittle: 函数
star: true
icon: pen-to-square
date: 2024-9-18
category:
  - 算法笔记
timeline: false 
watermark:
  width: 200
  height: 200
  content: uestc-lzy
  opacity: 0.5
tag:
  - 论文
  - 注意力机制
  - 编码器解码器 
---
# Transformer
本文从相关知识开始逐步讲解transformer
<!-- more -->
## 相关知识
本章节主要介绍编码器与解码器，注意力机制的起源，也是transformer论文的灵感来源。
### 编码器与解码器
编码器-解码器（Encoder-Decoder）架构在不同的领域和任务中有不同的首次提及。在自然语言处理领域，特别是在序列到序列（seq2seq）模型中，这种架构被广泛应用于机器翻译等任务。而在计算机视觉领域，如**图像分割**任务中，SegNet是最早提出并使用编码器-解码器架构的网络之一，其论文为《SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation》。
![alt text](image.png)
最初的编码器解码器使用的是基于cnn的结构，并用于图像分割，SegNet 的核心是一个编码器网络，它与 VGG16 网络的前 13 个卷积层在拓扑上是相同的，以及一个相应的解码器网络，其作用是将低分辨率的编码器特征图映射回原始输入分辨率的特征图，以便进行像素级的分类。SegNet 的创新之处在于解码器上采样低分辨率输入特征图的方式，它使用编码器最大池化步骤中计算的池化索引来执行**非线性上采样**，从而消除了上采样的学习需求。

### 注意力机制

Transformer原文中在介绍注意力机制的部分有下面这句话：

![](image-1.png)  

那么所谓的相加性质的注意力函数到底是怎么回事呢？在NLP相关论文对齐翻译NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE中首次提到这种注意力函数，论文首次提出了一种新的神经机器翻译方法。这种方法的核心思想是，不同于传统的编码器-解码器模型将整个源句子编码成一个固定长度的向量，该方法允许模型在生成目标词汇时，自动地（软搜索）寻找与预测目标词最相关的源句子部分，而不必显式地将这些部分形成硬片段。
![](image-2.png =300x500)  
上图为该论文中所使用方法模型的示意图，本文中将使用自顶向下的方法由解码器开始来解读。   
  
$$
p(y_{i} \mid y_{1}, \cdots, y_{i-1}, x) = g(y_{i-1}, s_{i}, c_{i})
$$
这是解码器最终输出目标词y的计算公式，可以看出他是跟RNN的式子很相似，应用了马尔可夫过程只与前一个输出目标词有关，那公式中剩余的两个参数$s_{i}$, $c_{i}$是什么呢？其中$s_{i}$比较好解释:
$$s_{i}=f\left(s_{i-1}, y_{i-1}, c_{i}\right)$$  
它就是RNN中中间的隐藏状态层，而剩下的参数$c_{i}$又是什么意思呢:
$$ c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j $$ 
这个在解码器中用到的这个公式就是这里的注意力机制，首先解释其中的参数$h_j$，它的出现是在下层的编码器中，而在编码器中并没有使用普通的RNN的结构，而是使用了双向RNN（BiRNN），每一部分的隐藏状态不但与先前隐藏状态有关，同时也与之后的隐藏状态有关，因此在论文中就给$h_j$起了一个新的名字叫注释（annotations），而从它的表现形式上来看，它是一个针对于输入词的一个向量，而这个向量又由之前的词和之后的词计算而来，代表了输入的词与句子中其它词的相近关系，因此叫注释。  
$$ \alpha_{ij} = \frac{\exp\left(e_{ij}\right)}{\sum_{k=1}^{T_x} \exp\left(e_{ik}\right)} $$  
其次是参数$\alpha_{ij}$，它由评分$e_{ij}$求得：
$$ e_{ij} = a(s_{i-1}, h_j) $$  
而评分通过a函数计算来计算编码器与解码器中隐藏状态的相关性，通常是用点乘的方式计算两个向量的相关性评分$e_{ij}$，而显然通过累乘$\alpha_{ij}$（评分占比）与$e_{ij}$（评分）再相加之后就实现了两种语言的对齐，即由输入词的注释转换为输出词的注释。









